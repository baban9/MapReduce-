{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multithreading of ids 561 Assignment 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhissGsMlSuWkToOSCYbB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baban9/MapReduce-/blob/master/multithreading_of_ids_561_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEQhuyVT-wTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "203c5792-38b5-4c50-f386-380de10d1ac8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ulLu6tAkO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "56521de1-9216-4264-b3c2-412b4bdbbb61"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 ')\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Assignment 1.ipynb'   'ids 561 Assignment 1.ipynb'   pride.txt\n",
            "'Frequency count.csv'   IDS561_lab2.ipynb\t     'Word count.csv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9e9FTBCvAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import threading\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds3X7rNP17vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cleaning(file_name):\n",
        "  # Some data cleaning jobs, such as  removing numbers, punctuations and special symbols, uppercase to lower case\n",
        "  line = open(file_name, 'rt')\n",
        "  text = line.read().strip()\n",
        "  \n",
        "  # removing numbers, punctuations, and special symbols\n",
        "  text_processing = re.sub('[^a-zA-Z\\.]', ' ', text)\n",
        "  text_only = re.sub(\"  \",\" \", text_processing)\n",
        "\n",
        "  # lower case characters \n",
        "  text_processed = text_only.lower()\n",
        "\n",
        "  return text_processed\n",
        "\n",
        "def data_split(processed_text):\n",
        "  # Split the dataset into two parts: Part1 includes the first 5000 lines of the text file, Part2 includes the rest text.\n",
        "  lines =  text_processed.split(\".\")\n",
        "\n",
        "  part1_data = lines[0:5000] \n",
        "  text_a = ' '.join([str(e) for e in part1_data]) \n",
        "\n",
        "  part2_data = lines[5001:]\n",
        "  text_b = ' '.join([str(e) for e in part2_data])\n",
        "\n",
        "  return text_a, text_b\n",
        "\n",
        "def Mapper(text):\n",
        "    mapped = []\n",
        "    for word in text.split(\" \"):\n",
        "      if word !=\"\":\n",
        "        mapped.append((word,1))\n",
        "    \n",
        "    return mapped\n",
        "\n",
        "\n",
        "def Sorting(mapped_1, mapped_2):\n",
        "  combined = mapped_1 + mapped_2\n",
        "  sorted_words = sorted(combined, key=lambda x: x[0])\n",
        "\n",
        "  return sorted_words\n",
        "\n",
        "def partition(words_sorted):\n",
        "  # partitioning \n",
        "\n",
        "  index = [x for x, y in enumerate(words_sorted) if y[0].startswith(\"n\")][0]\n",
        "  partition_a_m = words_sorted[0:index]\n",
        "  partition_n_z = words_sorted[index+1:]\n",
        "\n",
        "  return partition_a_m, partition_n_z\n",
        "\n",
        "\n",
        "def Reducer(sorted_list):\n",
        "    grouped = defaultdict(int) \n",
        "\n",
        "    for word in sorted_list:\n",
        "      grouped[word[0]] +=1\n",
        "\n",
        "    return grouped\n",
        "\n",
        "\n",
        "def output(word_dict_a_m, word_dict_n_z):\n",
        "  output = {**word_dict_a_m, **word_dict_n_z}\n",
        "  df = pd.DataFrame(output.items(),columns = ['word','frequency'])\n",
        "\n",
        "  return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  processed_text = data_cleaning('pride.txt')\n",
        "  text_5000, text_rest = data_split( processed_text)\n",
        "\n",
        "  # mapper threads\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    mapped_a = executor.submit(Mapper, text_5000)\n",
        "    mapped_b = executor.submit(Mapper, text_rest)\n",
        "\n",
        "  mapped_5000 = mapped_a.result()\n",
        "  mapped_rest = mapped_b.result()\n",
        "\n",
        "  words_sorted = Sorting(mapped_5000, mapped_rest)\n",
        "\n",
        "  partition_a_m, partition_n_z = partition(words_sorted)\n",
        "\n",
        "  # reducer threads\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    word_dict_thread_a= executor.submit(Reducer, partition_a_m)\n",
        "    word_dict_thread_n= executor.submit(Reducer, partition_n_z)\n",
        "\n",
        "  word_dict_a_m = word_dict_thread_a.result()\n",
        "  word_dict_n_z = word_dict_thread_n.result()\n",
        "\n",
        "  data = output(word_dict_a_m,word_dict_n_z)\n",
        "\n",
        "  df_sorted_word = data.copy() \n",
        "  df_sorted_freq = data.sort_values(['frequency'], ascending=False)\n",
        "\n",
        "  df_sorted_word.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Word count.csv\")\n",
        "  df_sorted_freq.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Frequency count.csv\")\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxj5Kj4FZMT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# approach 2 \n",
        "\n",
        "\n",
        "# Some data cleaning jobs, such as  removing numbers, punctuations and special symbols, uppercase to lower case\n",
        "line = open(\"pride.txt\", 'rt')\n",
        "text = line.read().strip()\n",
        "\n",
        "# removing numbers, punctuations, and special symbols\n",
        "text_processing = re.sub('[^a-zA-Z\\.]', ' ', text)\n",
        "text_only = re.sub(\"  \",\" \", text_processing)\n",
        "\n",
        "# lower case characters \n",
        "text_processed = text_only.lower()\n",
        "\n",
        "# Split the dataset into two parts: Part1 includes the first 5000 lines of the text file, Part2 includes the rest text.\n",
        "lines =  text_processed.split(\".\")\n",
        "\n",
        "part1_data = lines[0:5000] \n",
        "text_a = ' '.join([str(e) for e in part1_data]) \n",
        "\n",
        "part2_data = lines[5001:]\n",
        "text_b = ' '.join([str(e) for e in part2_data])\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  mapped_a = executor.submit(Mapper, text_a)\n",
        "  mapped_b = executor.submit(Mapper, text_b)\n",
        "\n",
        "mapped_5000 = mapped_a.result()\n",
        "mapped_rest = mapped_b.result()\n",
        "\n",
        "    \n",
        "# Sorting function \n",
        "combined = mapped_5000 + mapped_rest\n",
        "sorted_words = sorted(combined, key=lambda x: x[0])\n",
        "\n",
        "# partitioning \n",
        "\n",
        "index = [x for x, y in enumerate(sorted_words) if y[0].startswith(\"n\")][0]\n",
        "partition_a_m = sorted_words[0:index]\n",
        "partition_n_z = sorted_words[index+1:]\n",
        "\n",
        "# reducer \n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  word_dict_thread_a= executor.submit(Reducer, partition_a_m)\n",
        "  word_dict_thread_n= executor.submit(Reducer, partition_n_z)\n",
        "\n",
        "word_dict_a_m = word_dict_thread_a.result()\n",
        "word_dict_n_z = word_dict_thread_n.result()\n",
        "    \n",
        "output = {**word_dict_a_m, **word_dict_n_z}\n",
        "\n",
        "df = pd.DataFrame(output.items(),columns = ['word','frequency'])\n",
        "\n",
        "df_sorted_word = df.copy() \n",
        "df_sorted_freq = df.sort_values(['frequency'], ascending=False)\n",
        "\n",
        "df_sorted_word.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Word count.csv\")\n",
        "df_sorted_freq.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Frequency count.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF80Lbrr9PEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBSCdkwAUiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}