{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MapReduce - with multithreading.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baban9/MapReduce-/blob/master/MapReduce_with_multithreading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEQhuyVT-wTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fe5ea64-8f89-4e12-9d4f-6f639fdf1819"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ulLu6tAkO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d4507404-7c21-447f-cd00-8666ecbae98d"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 ')\n",
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Assignment 1.ipynb'\t      'multithreading of ids 561 Assignment 1.ipynb'\n",
            "'Frequency count.csv'\t       pride.txt\n",
            "'ids 561 Assignment 1.ipynb'  'Word count.csv'\n",
            " IDS561_lab2.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9e9FTBCvAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "import threading\n",
        "\n",
        "import time\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds3X7rNP17vi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "def data_cleaning(file_name):\n",
        "  \"\"\"\n",
        "  This function preprocesses data for\n",
        "  - removing numbers\n",
        "  - removing punctuations and any special symbols\n",
        "  - converting uppercase to lower case words\n",
        "\n",
        "  -- input - file name in .txt format \n",
        "  -- output - preprocessing of the text in the .txt file\n",
        "  \"\"\"\n",
        "\n",
        "  line = open(file_name, 'rt')\n",
        "  text = line.read().strip()\n",
        "  \n",
        "  # removing numbers, punctuations, and special symbols\n",
        "  text_processing = re.sub('[^a-zA-Z\\.]', ' ', text)\n",
        "  text_only = re.sub(\"  \",\" \", text_processing)\n",
        "\n",
        "  # lower casing characters \n",
        "  text_processed = text_only.lower()\n",
        "\n",
        "  return text_processed\n",
        "\n",
        "def data_split(processed_text):\n",
        "  \"\"\"\n",
        "  This function splits the data into two parts\n",
        "  - Part1 includes the first 5000 lines of the text file, \n",
        "  - Part2 includes the rest text\n",
        "\n",
        "  -- input - processed text after data cleaning \n",
        "  -- output - data split into two parts\n",
        "  \"\"\"\n",
        "  lines =  processed_text.split(\".\")\n",
        "\n",
        "  part1_data = lines[0:5000] \n",
        "  text_a = ' '.join([str(e) for e in part1_data]) \n",
        "\n",
        "  part2_data = lines[5001:]\n",
        "  text_b = ' '.join([str(e) for e in part2_data])\n",
        "\n",
        "  return text_a, text_b\n",
        "\n",
        "# Mapper function\n",
        "  \"\"\"\n",
        "  This is Mapper function that produce a\n",
        "  set of key-value pairs for Part1 and\n",
        "  Part2 subsets respectively. \n",
        "\n",
        "  -- input - splited data into two parts by data split function\n",
        "  -- output - Mapped data containing key value pairs\n",
        "  \"\"\"\n",
        "def Mapper(text):\n",
        "    mapped = []\n",
        "    for word in text.split(\" \"):\n",
        "      if word !=\"\":\n",
        "        mapped.append((word,1))\n",
        "    \n",
        "    return mapped\n",
        "\n",
        " # Sorting\n",
        "  \n",
        "def Sorting(mapped_1, mapped_2):\n",
        "  \"\"\" This is Sorting function that will combine the output from \n",
        "  two mapper functions and sort them based on keys\n",
        "\n",
        "  -- input - Outputs from two mapper functions\n",
        "  -- output - Sorted Key-Value pairs\n",
        "  \"\"\"\n",
        "  combined = mapped_1 + mapped_2\n",
        "  sorted_words = sorted(combined, key=lambda x: x[0])\n",
        "\n",
        "  return sorted_words\n",
        "\n",
        "# partitioning \n",
        "  \"\"\"\n",
        "  This function will partition the tokens (i.e., words) starting with\n",
        "  letter (“n” to “z”), and the others (\"a\" to \"m\").\n",
        "\n",
        "  -- input - Sorted Key-Value pairs\n",
        "  -- output - Ordered Partitioned key and count of frequency of words pairs\n",
        "  \"\"\"\n",
        "def partition(words_sorted):\n",
        "\n",
        "  index = [x for x, y in enumerate(words_sorted) if y[0].startswith(\"n\")][0]\n",
        "  partition_a_m = words_sorted[0:index]\n",
        "  partition_n_z = words_sorted[index+1:]\n",
        "\n",
        "  return partition_a_m, partition_n_z\n",
        "\n",
        "# Reducer function\n",
        "  \"\"\"\n",
        "  This function will collect values from each of the Ordered Partitioned containing the key\n",
        "  and count the frequency of words.\n",
        "\n",
        "  -- input - Ordered Partitioned Key-Value pairs output from partition function\n",
        "  -- output - Aggregation of (intermediate key-word count frequency pair) into a smaller set of key-word count frequency pairs\n",
        "  \"\"\"\n",
        "def Reducer(sorted_list):\n",
        "    grouped = defaultdict(int) \n",
        "\n",
        "    for word in sorted_list:\n",
        "      grouped[word[0]] +=1\n",
        "\n",
        "    return grouped\n",
        "\n",
        "# Combining output from both the reducers\n",
        "  \n",
        "def output(word_dict_a_m, word_dict_n_z):\n",
        "  \"\"\"\n",
        "  This function will combine the output of the two\n",
        "  partitions together and will return words and frequency count of the words\n",
        "\n",
        "  -- input - Aggregated output from the two reducer function\n",
        "  -- output - Dataframe containing the word and frequency count of the words\n",
        "  \"\"\"\n",
        "  output = {**word_dict_a_m, **word_dict_n_z}\n",
        "  df = pd.DataFrame(output.items(),columns = ['word','frequency'])\n",
        "\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxj5Kj4FZMT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56471c7b-58e9-4965-fd7c-91c9d6e0ba8a"
      },
      "source": [
        "# Running Multithreading \n",
        "\"\"\"The multithreading starts here \n",
        "      with monitoring the time of execution start\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "  start_time = time.time()\n",
        "  \n",
        "  processed_text = data_cleaning('pride.txt')\n",
        "  text_5000, text_rest = data_split(processed_text)\n",
        "\n",
        "  # mapper threads\n",
        "  \"\"\"Using ThreadPoolExecuter on Mapper. The executer will submit the Mapper function \n",
        "     and the argument which is the split data from the data_split function\n",
        "  \"\"\"\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    mapped_a = executor.submit(Mapper, text_5000)\n",
        "    mapped_b = executor.submit(Mapper, text_rest)\n",
        "\n",
        "  mapped_5000 = mapped_a.result()\n",
        "  mapped_rest = mapped_b.result()\n",
        "\n",
        "  words_sorted = Sorting(mapped_5000, mapped_rest)\n",
        "\n",
        "  partition_a_m, partition_n_z = partition(words_sorted)\n",
        "\n",
        "  # reducer threads\n",
        "  \"\"\"Using ThreadPoolExecuter on Reducer. The executer will submit the Reducer function \n",
        "     and the argument which is the Ordered Partitioned key and count of frequency of words pairs\n",
        "  \"\"\"\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    word_dict_thread_a= executor.submit(Reducer, partition_a_m)\n",
        "    word_dict_thread_n= executor.submit(Reducer, partition_n_z)\n",
        "\n",
        "  word_dict_a_m = word_dict_thread_a.result()\n",
        "  word_dict_n_z = word_dict_thread_n.result()\n",
        "\n",
        "  data = output(word_dict_a_m,word_dict_n_z)\n",
        "\n",
        "  df_sorted_word = data.copy() \n",
        "  df_sorted_freq = data.sort_values(['frequency'], ascending=False)\n",
        "\n",
        "# Output Word count to CSV file\n",
        "  \"\"\"This function would write the Word count into a CSV file.\n",
        "      Input - sorted word dataframe \n",
        "      Output - Output CSV to the local shared drive\n",
        "  \"\"\"\n",
        "  df_sorted_word.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Word count.csv\")\n",
        "  df_sorted_freq.to_csv(\"/content/drive/Shared drives/IDS 561 - Big Data /Assignment 1 /Frequency count.csv\")\n",
        "\n",
        "\n",
        "# Execution time\n",
        "time.time() - start_time"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24154376983642578"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF80Lbrr9PEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoBSCdkwAUiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}